{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "main() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-840f015063f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: main() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "    脚本名: fasttext 训练\n",
    "    process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n",
    "    fast text. using: very simple model;n-gram to captrue location information;h-softmax to speed up training/inference\n",
    "Created on 2019-01-17\n",
    "@author:David Yisun\n",
    "@group:data\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import word2vec\n",
    "import pickle\n",
    "import h5py\n",
    "from fastText_model_multilabel import fastTextB as fastText\n",
    "\n",
    "# 配置gpu资源\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6,7'  # 使用 GPU 0\n",
    "\n",
    "\n",
    "#configuration\n",
    "FLAGS=tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"cache_file_h5py\", \"/home/huzhiling/demo/data/text-classify/text-classification/data/ieee_zhihu_cup/data.h5\",\"path of training/validation/test data.\")  #../data/sample_multiple_label.txt 多标签文件\n",
    "tf.app.flags.DEFINE_string(\"cache_file_pickle\", \"/home/huzhiling/demo/data/text-classify/text-classification/data/ieee_zhihu_cup/vocab_label.pik\", \"path of vocabulary and label files\")   #../data/sample_multiple_label.txt\n",
    "\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.001, \"learning rate\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 128, \"Batch size for training/evaluating.\")  # 512批处理的大小 32-->128\n",
    "tf.app.flags.DEFINE_integer(\"decay_steps\", 20000, \"how many steps before decay learning rate.\")  # 批处理的大小 32-->128 多少steps后开始衰减\n",
    "tf.app.flags.DEFINE_float(\"decay_rate\", 0.9, \"Rate of decay for learning rate.\")  # 0.5一次衰减多少  衰减比例\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"num_sampled\", 10, \"number of noise sampling\")  # 100  噪声取样\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\", \"./model\", \"checkpoint location for the model\")  # 模型保存位置\n",
    "tf.app.flags.DEFINE_integer(\"sentence_len\", 200, \"max sentence length\")  # 句子长度 多少个词\n",
    "tf.app.flags.DEFINE_integer(\"embed_size\", 128, \"embedding size\")  # 128 embedding size\n",
    "tf.app.flags.DEFINE_boolean(\"is_training\", True, \"is traning.true:tranining,false:testing/inference\")  # 训练还是推断\n",
    "tf.app.flags.DEFINE_integer(\"num_epochs\", 25, \"embedding size\")  # epoch 数量\n",
    "tf.app.flags.DEFINE_integer(\"validate_every\", 1, \"Validate every validate_every epochs.\")  # 每10轮做一次验证\n",
    "\n",
    "tf.app.flags.DEFINE_boolean(\"use_embedding\", False, \"whether to use embedding or not.\")  # 是否使用外部embedding\n",
    "\n",
    "#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n",
    "\n",
    "\n",
    "def load_data(cache_file_h5py, cache_file_pickle):\n",
    "    \"\"\"\n",
    "    load data from h5py and pickle cache files, which is generate by take step by step of pre-processing.ipynb\n",
    "    :param cache_file_h5py:\n",
    "    :param cache_file_pickle:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not os.path.exists(cache_file_h5py) or not os.path.exists(cache_file_pickle):\n",
    "        raise RuntimeError(\"############################ERROR##############################\\n. \"\n",
    "                           \"please download cache file, it include training data and vocabulary & labels. \"\n",
    "                           \"link can be found in README.md\\n download zip file, unzip it, then put cache files as FLAGS.\"\n",
    "                           \"cache_file_h5py and FLAGS.cache_file_pickle suggested location.\")\n",
    "    print(\"INFO. cache file exists. going to load cache file\")\n",
    "    f_data = h5py.File(cache_file_h5py, 'r')\n",
    "    print(\"f_data.keys:\", list(f_data.keys()))\n",
    "    train_X=f_data['train_X']  # np.array(\n",
    "    print(\"train_X.shape:\", train_X.shape)\n",
    "    train_Y=f_data['train_Y']  # np.array(\n",
    "    print(\"train_Y.shape:\", train_Y.shape, \";\")\n",
    "    vaild_X=f_data['vaild_X']  # np.array(\n",
    "    valid_Y=f_data['valid_Y']  # np.array(\n",
    "    test_X=f_data['test_X']  # np.array(\n",
    "    test_Y=f_data['test_Y']  # np.array(\n",
    "\n",
    "    word2index, label2index = None, None\n",
    "    with open(cache_file_pickle, 'rb') as data_f_pickle:\n",
    "        word2index, label2index = pickle.load(data_f_pickle)\n",
    "    print(\"INFO. cache file load successful...\")\n",
    "    return word2index, label2index, train_X, train_Y, vaild_X, valid_Y, test_X, test_Y\n",
    "\n",
    "# 赋值外部embedding\n",
    "def assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, fast_text):\n",
    "    print(\"using pre-trained word emebedding.started...\")\n",
    "    # word2vecc=word2vec.load('word_embedding.txt') #load vocab-vector fiel.word2vecc['w91874']\n",
    "    word2vec_model = word2vec.load('zhihu-word2vec-multilabel.bin-100', kind='bin')\n",
    "    word2vec_dict = {}\n",
    "    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n",
    "        word2vec_dict[word] = vector\n",
    "    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n",
    "    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:'PAD'\n",
    "    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n",
    "    count_exist = 0\n",
    "    count_not_exist = 0\n",
    "    for i in range(1, vocab_size):  # loop each word\n",
    "        word = vocabulary_index2word[i]  # get a word\n",
    "        embedding = None\n",
    "        try:\n",
    "            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n",
    "        except Exception:\n",
    "            embedding = None\n",
    "        if embedding is not None:  # the 'word' exist a embedding\n",
    "            word_embedding_2dlist[i] = embedding\n",
    "            count_exist = count_exist + 1  # assign array to this word.\n",
    "        else:  # no embedding for this word\n",
    "            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size)\n",
    "            count_not_exist = count_not_exist + 1  # init a random value for the word.\n",
    "    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n",
    "    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n",
    "    t_assign_embedding = tf.assign(fast_text.Embedding,\n",
    "                                   word_embedding)  # assign this value to our embedding variables of our model.\n",
    "    sess.run(t_assign_embedding)\n",
    "    print(\"word. exists embedding:\", count_exist, \" ;word not exist embedding:\", count_not_exist)\n",
    "    print(\"using pre-trained word emebedding.ended...\")\n",
    "\n",
    "# 在验证集上做验证，报告损失、精确度\n",
    "def do_eval(sess, fast_text, evalX, evalY, batch_size, vocabulary_index2word_label):  # evalY1999\n",
    "    evalX = evalX[0:3000]\n",
    "    evalY = evalY[0:3000]\n",
    "    number_examples, labels = evalX.shape\n",
    "    print(\"number_examples for validation:\", number_examples)\n",
    "    eval_loss, eval_acc, eval_counter = 0.0, 0.0, 0\n",
    "    batch_size = 1\n",
    "    for start, end in zip(range(0, number_examples, batch_size), range(batch_size, number_examples,batch_size)):\n",
    "        evalY_batch = process_labels(evalY[start:end])   # 整理成 [类别x， 类别x， 类别y， 类别y， 类别z] 的形式\n",
    "        curr_eval_loss, logit = sess.run([fast_text.loss_val, fast_text.logits],  # curr_eval_acc-->fast_text.accuracy\n",
    "                                          feed_dict={fast_text.sentence: evalX[start:end],\n",
    "                                                     fast_text.labels_l1999: evalY[start:end]}) #,fast_text.labels_l1999:evalY1999[start:end]\n",
    "        # print(\"do_eval.logits_\", logits_.shape)\n",
    "        label_list_top5 = get_label_using_logits(logit[0], vocabulary_index2word_label)  # 选出概率最大的前5个label\n",
    "        curr_eval_acc = calculate_accuracy(list(label_list_top5), evalY_batch[0], eval_counter)  # evalY[start:end][0]\n",
    "        print('evalY_batch shape: {0} * {1}'.format(evalY_batch.shape))\n",
    "        eval_loss, eval_counter, eval_acc = eval_loss+curr_eval_loss, eval_counter+1, eval_acc+curr_eval_acc\n",
    "\n",
    "    return eval_loss/float(eval_counter), eval_acc/float(eval_counter)\n",
    "\n",
    "def process_labels(trainY_batch, require_size=5, number=None):\n",
    "    \"\"\"\n",
    "    限制label的输出维度  即 top N\n",
    "    process labels to get fixed size labels given a spense label\n",
    "    :param trainY_batch:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #print(\"###trainY_batch:\",trainY_batch)\n",
    "    num_examples, _ = trainY_batch.shape\n",
    "    trainY_batch_result = np.zeros((num_examples, require_size), dtype=int)\n",
    "\n",
    "    for index in range(num_examples):\n",
    "        y_list_sparse = trainY_batch[index]\n",
    "        y_list_dense = [i for i, label in enumerate(y_list_sparse) if int(label) == 1]  # 选出每个样本所有标签 label向量中为1的\n",
    "        y_list = process_label_to_algin(y_list_dense, require_size=require_size)\n",
    "        trainY_batch_result[index] = y_list\n",
    "        if number is not None and number%30 == 0:\n",
    "            pass\n",
    "            #print(\"####0.y_list_sparse:\",y_list_sparse)\n",
    "            #print(\"####1.y_list_dense:\",y_list_dense)\n",
    "            #print(\"####2.y_list:\",y_list) # 1.label_index: [315] ;2.y_list: [315, 315, 315, 315, 315] ;3.y_list: [0. 0. 0. ... 0. 0. 0.]\n",
    "    if number is not None and number % 30 == 0:\n",
    "        #print(\"###3trainY_batch_result:\",trainY_batch_result)\n",
    "        pass\n",
    "    return trainY_batch_result\n",
    "\n",
    "def process_label_to_algin(ys_list, require_size=5):\n",
    "    \"\"\"\n",
    "    given a list of labels, process it to fixed size('require_size')\n",
    "    :param ys_list: a list\n",
    "    :return: a list\n",
    "    \"\"\"\n",
    "    ys_list_result = [0 for x in range(require_size)]\n",
    "    if len(ys_list) >= require_size:  # 超长\n",
    "        ys_list_result = ys_list[0:require_size]\n",
    "    else:  # 太短\n",
    "       if len(ys_list) == 1:\n",
    "           ys_list_result = [ys_list[0] for x in range(require_size)]\n",
    "       elif len(ys_list) == 2:\n",
    "           ys_list_result = [ys_list[0], ys_list[0], ys_list[0], ys_list[1], ys_list[1]]\n",
    "       elif len(ys_list) == 3:\n",
    "           ys_list_result = [ys_list[0], ys_list[0], ys_list[1], ys_list[1], ys_list[2]]\n",
    "       elif len(ys_list) == 4:\n",
    "           ys_list_result = [ys_list[0], ys_list[0], ys_list[1], ys_list[2], ys_list[3]]\n",
    "    return ys_list_result\n",
    "\n",
    "#从logits中取出前五 get label using logits  返回list的index\n",
    "def get_label_using_logits(logits, vocabulary_index2word_label, top_number=5):\n",
    "    index_list = np.argsort(logits)[-top_number:]  # 排序返回index 默认从小到大\n",
    "    index_list = index_list[::-1]\n",
    "    #label_list=[]\n",
    "    #for index in index_list:\n",
    "    #    label=vocabulary_index2word_label[index]\n",
    "    #    label_list.append(label) #('get_label_using_logits.label_list:', [u'-3423450385060590478', u'2838091149470021485', u'-3174907002942471215', u'-1812694399780494968', u'6815248286057533876'])\n",
    "    return index_list\n",
    "\n",
    "#统计预测的准确率\n",
    "def calculate_accuracy(labels_predicted, labels, eval_counter):\n",
    "    if eval_counter<10:\n",
    "        print(\"labels_predicted:\", labels_predicted, \" ;labels:\",labels)\n",
    "    count = 0\n",
    "    label_dict = {x: x for x in labels}\n",
    "    for label_predict in labels_predicted:\n",
    "        flag = label_dict.get(label_predict, None)\n",
    "        if flag is not None:\n",
    "            count = count + 1\n",
    "    return count / len(labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1.load_data\n",
    "    # word2index 词表\n",
    "    # label2index label id\n",
    "    # trainX 训练集  batch * sentence_len 句子长度\n",
    "\n",
    "    trainX, trainY, testX, testY = None, None, None, None\n",
    "    word2index, label2index, trainX, trainY, vaildX, vaildY, testX, testY = load_data(FLAGS.cache_file_h5py, FLAGS.cache_file_pickle)  #\n",
    "    index2label = {v: k for k, v in label2index.items()}  # label的id\n",
    "    vocab_size = len(word2index)   # 词表大小\n",
    "    print(\"cnn_model.vocab_size:\", vocab_size)\n",
    "    num_classes = len(label2index)  # 类别大小\n",
    "    print(\"num_classes:\", num_classes)\n",
    "    num_examples, FLAGS.sentence_len = trainX.shape  # 获取 input 的 shape\n",
    "    print(\"num_examples of training:\", num_examples, \"    sentence_len:\", FLAGS.sentence_len)\n",
    "\n",
    "    # 2. create session\n",
    "    config = tf.ConfigProto()    # tf.ConfigProto一般用在创建session的时候。用来对session进行参数配置\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        # Instantiate Model\n",
    "        fast_text=fastText(num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.num_sampled,\n",
    "                           FLAGS.sentence_len, vocab_size, FLAGS.embed_size, FLAGS.is_training)\n",
    "\n",
    "        # Initialize Save\n",
    "        saver = tf.train.Saver()\n",
    "        if os.path.exists(os.path.join(FLAGS.ckpt_dir, \"checkpoint\")):\n",
    "            print(\"Restoring Variables from Checkpoint\")\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "        else:\n",
    "            print('Initializing Variables')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            if FLAGS.use_embedding:  # load pre-trained word embedding 是否用外部训练的embedding\n",
    "                vocabulary_index2word = {v: k for k, v in word2index.items()}\n",
    "                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, fast_text)\n",
    "\n",
    "        curr_epoch = sess.run(fast_text.epoch_step)\n",
    "\n",
    "        # 3.feed data & training\n",
    "        number_of_training_data = len(trainX)  # 训练数据量\n",
    "        batch_size = FLAGS.batch_size\n",
    "        for epoch in range(curr_epoch, FLAGS.num_epochs):\n",
    "            loss, acc, counter = 0.0, 0.0, 0\n",
    "            for start, end in zip(range(0, number_of_training_data, batch_size), range(batch_size, number_of_training_data, batch_size)):\n",
    "                curr_loss, current_l2_loss, _ = sess.run([fast_text.loss_val, fast_text.l2_losses, fast_text.train_op],\n",
    "                                                         feed_dict={fast_text.sentence: trainX[start: end],\n",
    "                                                                    fast_text.labels_l1999: trainY[start: end]})\n",
    "\n",
    "                if epoch == 0 and counter == 0:\n",
    "                    print(\"trainX[start:end]:\", trainX[start:end])  # 2d-array. each element slength is a 100.\n",
    "                    print(\"train_Y_batch:\",\n",
    "                          trainY[start:end])  # a list,each element is a list.element:may be has 1,2,3,4,5 labels.\n",
    "\n",
    "                loss, counter = loss + curr_loss, counter + 1  # acc+curr_acc loss累加 counter：统计每个epoch的batch数\n",
    "                if counter % 50 == 0:\n",
    "                    print(\"Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tL2 Loss:%.3f\" % (epoch, counter, loss / float(counter), current_l2_loss))\n",
    "                    # \\tTrain Accuracy:%.3f--->,acc/float(counter)\n",
    "\n",
    "                if start % (1000 * FLAGS.batch_size) == 0:  # 每1000个batch 做一次验证\n",
    "                    eval_loss, eval_accuracy = do_eval(sess, fast_text, vaildX, vaildY, batch_size,\n",
    "                                                       index2label)  # testY1999,eval_acc\n",
    "                    print(\"Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f\" % (epoch, eval_loss, eval_accuracy))\n",
    "                    # ,\\tValidation Accuracy: %.3f--->eval_acc\n",
    "\n",
    "                    # save model to checkpoint 保存模型  每6000 * FLAGS.batch_size 保存一次\n",
    "                    if start % (6000 * FLAGS.batch_size) == 0:\n",
    "                        print(\"Going to save checkpoint.\")\n",
    "                        save_path = FLAGS.ckpt_dir + \"model.ckpt\"\n",
    "                        saver.save(sess, save_path, global_step=epoch)  # fast_text.epoch_step\n",
    "\n",
    "            # epoch increment\n",
    "            print(\"going to increment epoch counter....\")\n",
    "            sess.run(fast_text.epoch_increment)  # 记录epoch数\n",
    "\n",
    "            # 4.validation\n",
    "            print(\"epoch:\", epoch, \"validate_every:\", FLAGS.validate_every, \"validate or not:\", (epoch % FLAGS.validate_every==0))\n",
    "            if epoch % FLAGS.validate_every == 0:\n",
    "                eval_loss, eval_accuracy = do_eval(sess, fast_text, vaildX, vaildY, batch_size,\n",
    "                                                   index2label)  # testY1999,eval_acc\n",
    "                print(\"Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f\" % (epoch, eval_loss, eval_accuracy))  # ,\\tValidation Accuracy: %.3f--->eval_acc\n",
    "                # save model to checkpoint\n",
    "                print(\"Going to save checkpoint.\")\n",
    "                save_path=FLAGS.ckpt_dir+\"model.ckpt\"\n",
    "                saver.save(sess,save_path,global_step=epoch)  # fast_text.epoch_step\n",
    "\n",
    "            # 5.最后在测试集上做测试，并报告测试准确率 Test\n",
    "            test_loss, test_acc = do_eval(sess, fast_text, testX, testY, batch_size, index2label)  # testY1999\n",
    "            print('测试集\\nloss:{loss}    acc:{acc}'.format(loss=test_loss, acc=test_acc))\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
